<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
    <meta charset="utf-8"/>
    <title>Forward, Loss, Backward, Step</title>
    
    <meta name="description" content="Welcome to my awesome blog">
    <meta name="keywords" content="">
    
    <link rel="canonical" href="https://jglezt.github.io/">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href="//fonts.googleapis.com/css?family=Alegreya:400italic,700italic,400,700" rel="stylesheet"
          type="text/css">
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.0/css/bootstrap.min.css">
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.7.0/styles/default.min.css">
    <link href="/css/screen.css" rel="stylesheet" type="text/css" />
</head>
<body>


<nav class="navbar navbar-default">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Forward, Loss, Backward, Step</a>
        </div>
        <div id="navbar" class="navbar-collapse collapse">
            <ul class="nav navbar-nav navbar-right">
                <li  class="active" ><a href="/">Home</a></li>
                <li
                ><a href="/archives/">Archives</a></li>
                
                <li
                >
                <a href="/pages-output/about/">About</a>
                </li>
                
                <li><a href="/feed.xml">RSS</a></li>
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
                        More <span class="caret"></span></a>
                    <ul class="dropdown-menu" role="menu">
                        <li class="dropdown-header">Links</li>
                        <!-- <li><a href="http://cryogenweb.org/docs/home.html">Cryogen Docs</a></li> -->
                        <!-- <li><a href="https://carmen.la/blog/archives/">Carmen's Blog</a></li> -->
                        
                        <li><a href="/pages-output/another-page/">Another Page</a></li>
                        

                        
                        <li class="divider"></li>
                        <li class="dropdown-header">Recent Posts</li>
                        
                        <li><a href="/posts-output/2020-12-13-attention/">Analysis of attention mechanism in the task of Machine translation</a></li>
                        
                        <li><a href="/posts-output/2020-10-22-grad_cam/">GRAD-CAM</a></li>
                        
                        

                        
                        <li class="divider"></li>
                        <li class="dropdown-header">Tags</li>
                        
                        <li><a href="/tags-output/nlp/">nlp</a></li>
                        
                        <li><a href="/tags-output/deep learning/">deep learning</a></li>
                        
                        <li><a href="/tags-output/gru/">gru</a></li>
                        
                        <li><a href="/tags-output/attention/">attention</a></li>
                        
                        <li><a href="/tags-output/object detection/">object detection</a></li>
                        
                        <li><a href="/tags-output/tensorflow/">tensorflow</a></li>
                        
                        <li><a href="/tags-output/object localization/">object localization</a></li>
                        
                        
                    </ul>
                </li>
            </ul>
        </div><!--/.nav-collapse -->
    </div><!--/.container-fluid -->
</nav>


<div class="container">


    <div class="row">
        <div class="col-lg-12">
            <div id="content">
                
<div id="post">
    <div class="post-header">
    <div id="post-meta" class="row">
        <div class="col-lg-6">December 13, 2020</div>
        
        <span class="col-lg-6 right">By: Javier Antonio Gonzalez-Trejo</span>
        
    </div>
    <h2>Analysis of attention mechanism in the task of Machine translation</h2>
</div>
<div>
    <ol class="toc"><li><a href="#original-notebook-by-joosthub">Original notebook by <a href="https://github.com/joosthub/PyTorchNLPBook/tree/master/chapters/chapter_8">joosthub</a></a></li><li><a href="#run-in-google-colab">Run in Google Colab</a></li><li><a href="#1-implent-the-bahdanau-soft-attention">1) Implent the Bahdanau Soft-Attention</a></li><ol><li><a href="#aditional-modification">Aditional modification</a></li><li><a href="#train-the-soft-attention-model">Train the soft attention model</a></li></ol><li><a href="#train-soft-attention-from-book">Train Soft Attention from book</a></li><li><a href="#3-performance-comparision-between-bahdanau-and-baseline-soft-attention">3) Performance comparision between Bahdanau and baseline Soft attention</a></li><li><a href="#2-visualize-bahdanau-soft-attention">2) Visualize Bahdanau Soft Attention.</a></li><li><a href="#3-visual-comparision-between-the-attentions">3) Visual comparision between the attentions</a></li><li><a href="#4-local-attention">4) Local attention</a></li><ol><li><a href="#training-local-attention">Training Local Attention</a></li></ol><li><a href="#6-performance-comparision-betten-local-attention-vs-soft-attention-of-the-book-and-bahdanau-soft-attention">6) Performance comparision betten Local attention vs soft attention of the book and Bahdanau soft attention</a></li><li><a href="#5-visualize-local-attention">5) Visualize local attention</a></li><li><a href="#6-visual-comparision-betten-local-attention-vs-soft-attention-of-the-book-and-bahdanau-soft-attention">6) Visual comparision betten Local attention vs soft attention of the book and Bahdanau soft attention</a></li><li><a href="#attention-model-conclusions">Attention model conclusions</a></li></ol>
    <h2 id="original-notebook-by-joosthub">Original notebook by <a href="https://github.com/joosthub/PyTorchNLPBook/tree/master/chapters/chapter_8">joosthub</a></h2><p>In this notebook, we will assess two different attention models and compare them between them and against the Soft attention model proposed in the book of "Natural Language Processing with PyTorch". For a fair comparison, we will use almost all hyper parameters used in Chapter 8 of the book, while making modifications for code legibility of maintaining the attention model requirements, for example, the use of the previous hidden state of the decoder for the Bahdanau attention model. Finally, we will decide which attention model is suitable for the Chat Bot task.</p><p>The points to evaluate are highlighted by a third level title followed by the number of the objective. The objectives signaled are only the ones that are weigthed.</p><pre><code class="python">import os
from argparse import Namespace
from collections import Counter
import json
import re
import string

import tqdm
import nltk
import numpy as np
import pandas as pd
import random as rnd
import torch
import torch.nn as nn
from torch.nn import functional as F
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from tqdm import notebook, tqdm

from nltk.translate import bleu_score
import seaborn as sns
import matplotlib.pyplot as plt

chencherry = bleu_score.SmoothingFunction()

if torch.cuda.is_available():
    device = torch.device('cuda')
else:
    device = torch.device('cpu')
</code></pre><h2 id="run-in-google-colab">Run in Google Colab</h2><p>We move all the dot py files to the same directory as our notebook in order to use them.</p><pre><code class="python">from google.colab import drive
drive.mount('/content/drive/')
</code></pre><pre><code>ModuleNotFoundErrorTraceback (most recent call last)

&lt;ipython-input-2-91874b305a32&gt; in &lt;module&gt;
----&gt; 1 from google.colab import drive
      2 drive.mount('/content/drive/')


ModuleNotFoundError: No module named 'google.colab'
</code></pre><pre><code class="python">ROOT_DIR = "/content/drive/MyDrive/semester_3/tarea_chat_bot_javier_antonio_gonzalez_trejo"
for file in os.listdir(ROOT_DIR):
  try:
    shutil.copy(os.path.join(ROOT_DIR, file), 
                os.path.join('.', file))
  except IsADirectoryError:
    shutil.copytree(os.path.join(ROOT_DIR, file), 
                  os.path.join('.', file))
</code></pre><pre><code>FileNotFoundErrorTraceback (most recent call last)

&lt;ipython-input-2-1f8d61420637&gt; in &lt;module&gt;
      1 ROOT_DIR = "/content/drive/MyDrive/semester_3/tarea_chat_bot_javier_antonio_gonzalez_trejo"
----&gt; 2 for file in os.listdir(ROOT_DIR):
      3   try:
      4     shutil.copy(os.path.join(ROOT_DIR, file), 
      5                 os.path.join('.', file))


FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/semester_3/tarea_chat_bot_javier_antonio_gonzalez_trejo'
</code></pre><pre><code class="python">from model import NMTEncoder, TerseAttention
from training import set_seed_everywhere, make_train_state,\
    update_train_state, normalize_sizes, compute_accuracy,\
    sequence_loss, train

from utils import sentence_from_indices, NMTSampler, create_dataset,\
    expand_filepaths_to_save_dir, handle_dirs, evaluate_model_blue_4,\
    visualize_attention

from dataset import NMTVectorizer, NMTDataset, generate_nmt_batches
</code></pre><h2 id="1-implent-the-bahdanau-soft-attention">1) Implent the Bahdanau Soft-Attention</h2><p>The attention mechanism proposed in the Chapter 8 in the book "Natural Language Processing with PyTorch" differs from the one proposed by Bahdanau in "Neural Machine Translation By Jointly to Align and Translate" in the alignment model; in the book they use a simple dot-product and in the paper, a fully conected neural network.  This neural network is composed firstly of two fully conected layers with linear activations for the current hidden state in the decoder $s_{i - 1}$ and the hidden state of the encoder $h_{j}$ that the hidden state in the decoder is comparing to. Both results are added and passed to a tanh activation and be finally reduced by a final fully conected layer which represnts the algiment of the hidden states $s_{i -1}$ and $h_{j}$
More formally we have:
$$ a(s_{i - 1}, h_{j}) = v_{a}<sup>{\intercal} tanh(W_{a}s_{i-1} + U_{a}h_{j}) \forall j \in # x$$
where $ W_{a} \in \mathbb{R}</sup>{n \times n}$, $U_{a} \in \mathbb{R}<sup>{n \times n}$, $v_{a} \in \mathbb{R}</sup>{n}$ and $n$ is equal to the hidden staten of the decoder.</p><p>For a fair comparision between the different attention models, and since we comparing attention models, we will modifified the weigth $U_{a}$ to be of dimention $\mathbb{R}^{n \times n}$ to accomodate the hidden state size of the decoder proposed in the book.</p><p>The main idea behind this implementation is to learn how to obtain the "alignment" that a a hidden state "s" has with a hidden state "h" in order to say that the hidden state "h" is relevant to translate a word.
Contrasting with the dot product, where the hidden states only align when a lot of their features are big and have the same sign.</p><h3 id="aditional-modification">Aditional modification</h3><ol><li>In the original code, there was a bug which prevented the model to be used without a target sentence. The number of times that the loop ran was given a by a target sequence length, but in deployment we do not have such target length. There exist many ways to aid the model, here, the method selected is to provide a maximum sequence length (50) if no target sequence length is provided.</li><li>The creation of an interface to better organize all the types of attentions implemented for this work. This interface has 'precalculate_encoder_features' which is executed before the GRU loop in the NMTDecoder to avoid redundant calculations, since the hidden states "h" of the encoder do not change at this step</li></ol><pre><code class="python">class AttentionInterface(nn.Module):
    def __init__(self):
        super(AttentionInterface, self).__init__()
    def forward(self, query_vector, encoder_state_vectors):
        pass
    def precalculate_encoder_features(self, encoder_state_vectors):
        pass
</code></pre><pre><code class="python">class BahdanauAttention(AttentionInterface):
    """Implementation of the Bahdanau Soft Attention

    Args:
       num_features: Dimentionality of the features of both hidden states $h$ and $s$ to calculate the alignment.
       size_encoder_vector: Size of the hidden state $h$
       size_query_vector: Size of the hidden state $s$
    """
    def __init__(self, num_features, size_encoder_vector, size_query_vector):
        super(BahdanauAttention, self).__init__()
        self.encoder_weigths = nn.Linear(size_encoder_vector, num_features)
        self.query_weigths = nn.Linear(size_query_vector, num_features)
        self.aligment_weigths = nn.Linear(num_features, 1)

    def precalculate_encoder_features(self, encoder_state_vectors):
        """The encoder hidden states does not change at decoding time, thus is better
        to calculate their features one for each decoding hidden state

        Args: encoder_state_vector"""

        self.encoder_state_features =\
            self.encoder_weigths(encoder_state_vectors)

        return self.encoder_state_features

    def forward(self, query_vector, encoder_state_vectors):
        """Calculate the context vector and the attention probability distribution.

        Args:
            query_vector: torch.Size([batch_size, hidden_size]
            encoder_state_vectors
        """

        # Unsqueeze in order to broadcast over the encoder features
        query_features = self.query_weigths(query_vector).unsqueeze(dim = 1)

        vector_scores = self.aligment_weigths(
            torch.tanh(query_features +\
                       self.encoder_state_features)).squeeze()

        vector_probabilities = F.softmax(vector_scores, dim=-1)

        context_vectors = torch.matmul(encoder_state_vectors.transpose(-2, -1),
                                   vector_probabilities.unsqueeze(dim=2)).squeeze()

        return context_vectors, vector_probabilities
</code></pre><p>The soft attention implementation of the book will in this notebook to save space, but they are available in the file model.py on this same directory.</p><p>First, we create the NMTDecoder for  models that follow the Luong methodology. (Soft attention from book and local attention.</p><pre><code class="python">class NMTDecoder(nn.Module):
    def __init__(self, num_embeddings, embedding_size, rnn_hidden_size, bos_index,
                 attention, attention_init_args):
        """
        Args:
            num_embeddings (int): number of embeddings is also the number of 
                unique words in target vocabulary 
            embedding_size (int): the embedding vector size
            rnn_hidden_sizes (int, int, int):
               num_features: number of features for the attention module, if any
               size_encoder_vector, size of the hidden state of the encoder
               size_query_vector, size of the hidden state of the decoder
            bos_index(int): begin-of-sequence index
            attention (nn.Module) Type of attention used.
            attention_init_args (list) Arguments used to instanciate
              the attention class
        """
        super(NMTDecoder, self).__init__()
        self._rnn_hidden_size = rnn_hidden_size
        self.target_embedding = nn.Embedding(num_embeddings=num_embeddings, 
                                             embedding_dim=embedding_size, 
                                             padding_idx=0)
        self.gru_cell = nn.GRUCell(embedding_size + rnn_hidden_size, 
                                   rnn_hidden_size)
        self.hidden_map = nn.Linear(rnn_hidden_size, rnn_hidden_size)
        self.classifier = nn.Linear(rnn_hidden_size * 2, num_embeddings)
        self.bos_index = bos_index
        self._sampling_temperature = 3

        # Create the Attention module
        self.attention = attention(*attention_init_args)

    def _init_indices(self, batch_size):
        """ return the BEGIN-OF-SEQUENCE index vector """
        return torch.ones(batch_size, dtype=torch.int64) * self.bos_index

    def _init_context_vectors(self, batch_size):
        """ return a zeros vector for initializing the context """
        return torch.zeros(batch_size, self._rnn_hidden_size)

    def forward(self, encoder_state, initial_hidden_state, target_sequence,
                sample_probability=0.0, max_sequence_size = 20):
        """The forward pass of the model

        Args:
            encoder_state (torch.Tensor): the output of the NMTEncoder
            initial_hidden_state (torch.Tensor): The last hidden state in the  NMTEncoder
            target_sequence (torch.Tensor): the target text data tensor
            sample_probability (float): the schedule sampling parameter
                probabilty of using model's predictions at each decoder step
        Returns:
            output_vectors (torch.Tensor): prediction vectors at each output step
        """
        if target_sequence is None:
            sample_probability = 1.0
            output_sequence_size = max_sequence_size
        else:
            # We are making an assumption there: The batch is on first
            # The input is (Batch, Seq)
            # We want to iterate over sequence so we permute it to (S, B)
            target_sequence = target_sequence.permute(1, 0)
            output_sequence_size = target_sequence.size(0)

        # use the provided encoder hidden state as the initial hidden state
        h_t = self.hidden_map(initial_hidden_state)

        batch_size = encoder_state.size(0)
        # initialize context vectors to zeros
        context_vectors = self._init_context_vectors(batch_size)
        # initialize first y_t word as BOS
        y_t_index = self._init_indices(batch_size)

        h_t = h_t.to(encoder_state.device)
        y_t_index = y_t_index.to(encoder_state.device)
        context_vectors = context_vectors.to(encoder_state.device)

        output_vectors = []
        self._cached_p_attn = []
        self._cached_ht = []
        self._cached_decoder_state = encoder_state.cpu().detach().numpy()

        # Precalculate the encoder features
        self.attention.precalculate_encoder_features(encoder_state)

        for i in range(output_sequence_size):
            # Schedule sampling is whe
            use_sample = np.random.random() &lt; sample_probability
            if not use_sample:
                y_t_index = target_sequence[i]
                
            # Step 1: Embed word and concat with previous context
            y_input_vector = self.target_embedding(y_t_index)
            rnn_input = torch.cat([y_input_vector, context_vectors], dim=1)

            # Step 2: Make a GRU step, getting a new hidden vector
            h_t = self.gru_cell(rnn_input, h_t)
            self._cached_ht.append(h_t.cpu().detach().numpy())
            
            # Step 3: Use the current hidden to attend to the encoder state
            context_vectors, p_attn = self.attention(encoder_state_vectors=encoder_state, 
                                                           query_vector=h_t)
            
            # auxillary: cache the attention probabilities for visualization
            self._cached_p_attn.append(p_attn.cpu().detach().numpy())
            
            # Step 4: Use the current hidden and context vectors to make a prediction to the next word
            prediction_vector = torch.cat((context_vectors, h_t), dim=1)
            score_for_y_t_index = self.classifier(F.dropout(prediction_vector, 0.3))
            
            if use_sample:
                p_y_t_index = F.softmax(score_for_y_t_index * self._sampling_temperature, dim=1)
                # _, y_t_index = torch.max(p_y_t_index, 1)
                y_t_index = torch.multinomial(p_y_t_index, 1).squeeze()
            
            # auxillary: collect the prediction scores
            output_vectors.append(score_for_y_t_index)
            
        output_vectors = torch.stack(output_vectors).permute(1, 0, 2)
        
        return output_vectors
</code></pre><p>Then, we create the sub class NTMDecoderBahdanau, override the forward function only. The new forward function will feed the previus hidden state to the attention module, which is one of the details of the Bahdanau Soft attention implementation</p><pre><code class="python">class NMTDecoderBahdanau(NMTDecoder):
    def __init__(self, num_embeddings, embedding_size, rnn_hidden_size, bos_index,
                 attention, attention_init_args):
        """
        Args:
            num_embeddings (int): number of embeddings is also the number of 
                unique words in target vocabulary 
            embedding_size (int): the embedding vector size
            rnn_hidden_sizes (int, int, int):
               num_features: number of features for the attention module, if any
               size_encoder_vector, size of the hidden state of the encoder
               size_query_vector, size of the hidden state of the decoder
            bos_index(int): begin-of-sequence index
            attention (nn.Module) Type of attention used.
            attention_init_args (list) Arguments used to instanciate
              the attention class
        """
        super(NMTDecoderBahdanau, self).__init__(num_embeddings, embedding_size,
                                                 rnn_hidden_size, bos_index,
                                                 attention, attention_init_args)
    def forward(self, encoder_state, initial_hidden_state, target_sequence,
                sample_probability=0.0, max_sequence_size = 20):
        """The forward pass of the model

        Args:
            encoder_state (torch.Tensor): the output of the NMTEncoder
            initial_hidden_state (torch.Tensor): The last hidden state in the  NMTEncoder
            target_sequence (torch.Tensor): the target text data tensor
            sample_probability (float): the schedule sampling parameter
                probabilty of using model's predictions at each decoder step
        Returns:
            output_vectors (torch.Tensor): prediction vectors at each output step
        """
        if target_sequence is None:
            sample_probability = 1.0
            output_sequence_size = max_sequence_size
        else:
            # We are making an assumption there: The batch is on first
            # The input is (Batch, Seq)
            # We want to iterate over sequence so we permute it to (S, B)
            target_sequence = target_sequence.permute(1, 0)
            output_sequence_size = target_sequence.size(0)

        # use the provided encoder hidden state as the initial hidden state
        h_t = self.hidden_map(initial_hidden_state)

        batch_size = encoder_state.size(0)
        # initialize context vectors to zeros
        context_vectors = self._init_context_vectors(batch_size)
        # initialize first y_t word as BOS
        y_t_index = self._init_indices(batch_size)

        h_t = h_t.to(encoder_state.device)
        y_t_index = y_t_index.to(encoder_state.device)
        context_vectors = context_vectors.to(encoder_state.device)

        output_vectors = []
        self._cached_p_attn = []
        self._cached_ht = []
        self._cached_decoder_state = encoder_state.cpu().detach().numpy()

        # Precalculate the encoder features
        self.attention.precalculate_encoder_features(encoder_state)

        for i in range(output_sequence_size):
            # Schedule sampling is whe
            use_sample = np.random.random() &lt; sample_probability
            if not use_sample:
                y_t_index = target_sequence[i]

            # Step 1: Use the current hidden to attend to the encoder state
            context_vectors, p_attn = self.attention(encoder_state_vectors=encoder_state, 
                                                           query_vector=h_t)

            # auxillary: cache the attention probabilities for visualization
            self._cached_p_attn.append(p_attn.cpu().detach().numpy())

            # Step 2: Embed word and concat with previous context
            y_input_vector = self.target_embedding(y_t_index)
            rnn_input = torch.cat([y_input_vector, context_vectors], dim=1)

            # Step 3: Make a GRU step, getting a new hidden vector
            h_t = self.gru_cell(rnn_input, h_t)
            self._cached_ht.append(h_t.cpu().detach().numpy())

            # Step 4: Use the current hidden and context vectors to make a prediction to the next word
            prediction_vector = torch.cat((context_vectors, h_t), dim=1)
            score_for_y_t_index = self.classifier(F.dropout(prediction_vector, 0.3))
            
            if use_sample:
                p_y_t_index = F.softmax(score_for_y_t_index * self._sampling_temperature, dim=1)
                # _, y_t_index = torch.max(p_y_t_index, 1)
                y_t_index = torch.multinomial(p_y_t_index, 1).squeeze()
            
            # auxillary: collect the prediction scores
            output_vectors.append(score_for_y_t_index)
            
        output_vectors = torch.stack(output_vectors).permute(1, 0, 2)
        
        return output_vectors
</code></pre><p>Now, we implement the NMTModel adding an if, for different impelemtations of the decoder</p><pre><code class="python">class NMTModel(nn.Module):
    """ The Neural Machine Translation Model """
    def __init__(self, source_vocab_size, source_embedding_size,
                 target_vocab_size, target_embedding_size, encoding_size,
                 target_bos_index, attention, attention_init_args, decoder):
        """
        Args:
            source_vocab_size (int): number of unique words in source language
            source_embedding_size (int): size of the source embedding vectors
            target_vocab_size (int): number of unique words in target language
            target_embedding_size (int): size of the target embedding vectors
            attention (nn.Module): Class that implements the type of attention to use.
                it has as arguments (encoder_state_vectors, query vector) and returns
                the context_vectors and the vector_scores.
            attention_init_args (list) Arguments used to instanciate
              the attention class.
        """
        super(NMTModel, self).__init__()
        self.encoder = NMTEncoder(num_embeddings=source_vocab_size,
                                  embedding_size=source_embedding_size,
                                  rnn_hidden_size=encoding_size)
        decoding_size = encoding_size * 2
        
        # Here, we use different implementations of the decoder
        self.decoder = decoder(num_embeddings=target_vocab_size,
                                  embedding_size=target_embedding_size,
                                  rnn_hidden_size=decoding_size,
                                  bos_index=target_bos_index,
                                  attention=attention,
                                  attention_init_args=attention_init_args)

    def forward(self, x_source, x_source_lengths, target_sequence, sample_probability=0.0):
        """The forward pass of the model

        Args:
            x_source (torch.Tensor): the source text data tensor.
                x_source.shape should be (batch, vectorizer.max_source_length)
            x_source_lengths torch.Tensor): the length of the sequences in x_source
            target_sequence (torch.Tensor): the target text data tensor
            sample_probability (float): the schedule sampling parameter
                probabilty of using model's predictions at each decoder step
        Returns:
            decoded_states (torch.Tensor): prediction vectors at each output step
        """
        encoder_state, final_hidden_states = self.encoder(x_source, x_source_lengths)

        decoded_states = self.decoder(encoder_state=encoder_state,
                                      initial_hidden_state=final_hidden_states,
                                      target_sequence=target_sequence,
                                      sample_probability=sample_probability)
        return decoded_states
</code></pre><h3 id="train-the-soft-attention-model">Train the soft attention model</h3><p>Next, we will train the model.</p><pre><code class="python">args = Namespace(dataset_csv="data/nmt/simplest_eng_fra.csv",
                 vectorizer_file="vectorizer.json",
                 model_state_file="bahdanau_soft_attention.pth",
                 save_dir="model_storage/attention",
                 reload_from_files=False,
                 expand_filepaths_to_save_dir=True,
                 cuda=True,
                 seed=1337,
                 learning_rate=5e-4,
                 batch_size=32,
                 num_epochs=100,
                 early_stopping_criteria=5,
                 source_embedding_size=24,
                 target_embedding_size=24,
                 encoding_size=32,
                 catch_keyboard_interrupt=True,
                 attention=BahdanauAttention,
                 attention_init_args=[64, 64, 64],
                 decoder=NMTDecoderBahdanau)

# handle dirs
handle_dirs(args.save_dir)

args = expand_filepaths_to_save_dir(args)

dataset = create_dataset(args)
vectorizer = dataset.get_vectorizer()

bahdanau_attention_model =\
     NMTModel(source_vocab_size=len(vectorizer.source_vocab),
              source_embedding_size=args.source_embedding_size,
              target_vocab_size=len(vectorizer.target_vocab),
              target_embedding_size=args.target_embedding_size, 
              encoding_size=args.encoding_size,
              target_bos_index=vectorizer.target_vocab.begin_seq_index,
              attention=args.attention,
              attention_init_args=args.attention_init_args,
              decoder=args.decoder)
</code></pre><pre><code>Expanded filepaths: 
	model_storage/attention/vectorizer.json
	model_storage/attention/bahdanau_soft_attention.pth
</code></pre><pre><code class="python">#bahdanau_attention_model,\
#    bahdanau_attention_state = train(args, dataset, bahdanau_attention_model)
# Save the model
#torch.save(bahdanau_attention_model.state_dict(), args.model_state_file)
</code></pre><pre><code>Using CUDA: True
New model



HBox(children=(FloatProgress(value=0.0, description='training routine', style=ProgressStyle(description_width=…



HBox(children=(FloatProgress(value=0.0, description='split=train', max=285.0, style=ProgressStyle(description_…



HBox(children=(FloatProgress(value=0.0, description='split=val', max=61.0, style=ProgressStyle(description_wid…


100%|██████████| 100/100 [22:08&lt;00:00, 13.29s/it]
</code></pre><pre><code class="python">bahdanau_attention_model.load_state_dict(torch.load(args.model_state_file))
</code></pre><pre><code>&lt;All keys matched successfully&gt;
</code></pre><p>Since we already have the model, we can also train the Bahdanau soft attention using the Luong decoder.</p><pre><code class="python">args = Namespace(dataset_csv="data/nmt/simplest_eng_fra.csv",
                 vectorizer_file="vectorizer.json",
                 model_state_file="bahdanau_soft_attentionv2.pth",
                 save_dir="model_storage/attention",
                 reload_from_files=False,
                 expand_filepaths_to_save_dir=True,
                 cuda=True,
                 seed=1337,
                 learning_rate=5e-4,
                 batch_size=32,
                 num_epochs=100,
                 early_stopping_criteria=5,
                 source_embedding_size=24,
                 target_embedding_size=24,
                 encoding_size=32,
                 catch_keyboard_interrupt=True,
                 attention=BahdanauAttention,
                 attention_init_args=[64, 64, 64],
                 decoder=NMTDecoder)

# handle dirs
handle_dirs(args.save_dir)

args = expand_filepaths_to_save_dir(args)

dataset = create_dataset(args)
vectorizer = dataset.get_vectorizer()

bahdanau_attention_modelv2 =\
     NMTModel(source_vocab_size=len(vectorizer.source_vocab),
              source_embedding_size=args.source_embedding_size,
              target_vocab_size=len(vectorizer.target_vocab),
              target_embedding_size=args.target_embedding_size, 
              encoding_size=args.encoding_size,
              target_bos_index=vectorizer.target_vocab.begin_seq_index,
              attention=args.attention,
              attention_init_args=args.attention_init_args,
              decoder=args.decoder)
</code></pre><pre><code>Expanded filepaths: 
	model_storage/attention/vectorizer.json
	model_storage/attention/bahdanau_soft_attentionv2.pth
</code></pre><pre><code class="python">#bahdanau_attention_modelv2,\
#    bahdanau_attention_statev2 = train(args, dataset, bahdanau_attention_modelv2)
# Save the model
#torch.save(bahdanau_attention_modelv2.state_dict(), args.model_state_file)
</code></pre><pre><code class="python">bahdanau_attention_modelv2.load_state_dict(torch.load(args.model_state_file))
</code></pre><pre><code>&lt;All keys matched successfully&gt;
</code></pre><h2 id="train-soft-attention-from-book">Train Soft Attention from book</h2><p>In order to assess the soft attention performance, we need to train the baseline implementation given by the book of NLP.</p><pre><code class="python">args = Namespace(dataset_csv="data/nmt/simplest_eng_fra.csv",
                 vectorizer_file="vectorizer.json",
                 model_state_file="soft_attention.pth",
                 save_dir="model_storage/attention",
                 reload_from_files=False,
                 expand_filepaths_to_save_dir=True,
                 cuda=True,
                 seed=1337,
                 learning_rate=5e-4,
                 batch_size=32,
                 num_epochs=100,
                 early_stopping_criteria=5,
                 source_embedding_size=24,
                 target_embedding_size=24,
                 encoding_size=32,
                 catch_keyboard_interrupt=True,
                 attention=TerseAttention,
                 attention_init_args=[],
                 decoder=NMTDecoder)

# handle dirs
handle_dirs(args.save_dir)

args = expand_filepaths_to_save_dir(args)

dataset = create_dataset(args)
vectorizer = dataset.get_vectorizer()

soft_attention_model =\
    NMTModel(source_vocab_size=len(vectorizer.source_vocab),
             source_embedding_size=args.source_embedding_size,
             target_vocab_size=len(vectorizer.target_vocab),
             target_embedding_size=args.target_embedding_size, 
             encoding_size=args.encoding_size,
             target_bos_index=vectorizer.target_vocab.begin_seq_index,
             attention=args.attention,
             attention_init_args=args.attention_init_args,
             decoder=args.decoder)
</code></pre><pre><code>Expanded filepaths: 
	model_storage/attention/vectorizer.json
	model_storage/attention/soft_attention.pth
</code></pre><pre><code class="python">#soft_attention_model,\
#    soft_attention_state = train(args, dataset, soft_attention_model)
# Save the model
#torch.save(soft_attention_model.state_dict(), args.model_state_file)
</code></pre><pre><code class="python">soft_attention_model.load_state_dict(torch.load(args.model_state_file))
</code></pre><pre><code>&lt;All keys matched successfully&gt;
</code></pre><h2 id="3-performance-comparision-between-bahdanau-and-baseline-soft-attention">3) Performance comparision between Bahdanau and baseline Soft attention</h2><p>First we compare the three models at a global level using the BLEU-4</p><pre><code class="python">set_seed_everywhere(args.seed, args.cuda)
</code></pre><pre><code class="python"># Soft attention
evaluate_model_blue_4(soft_attention_model, dataset, args)
</code></pre><pre><code>0.4088164938764507 0.4094979871241584
</code></pre><p><img src="/img/javier_antonio_gonzalez_trejo_attention_31_1.png" alt="png" /></p><pre><code class="python"># Bahdanau attention
evaluate_model_blue_4(bahdanau_attention_model, dataset, args)
</code></pre><pre><code>0.4790693537339051 0.4774714974640667
</code></pre><p><img src="/img/javier_antonio_gonzalez_trejo_attention_32_1.png" alt="png" /></p><p>The Bahdanau Soft attention implementation has better performance by at least by 3 BLEU points, which is a considerable margin of improvement. This migth be due the combination of the additive score, where the author argue that it has better performance than the dot score in some scenarios, and the ability of the model to learn the relevance a hidden unit $h$ in the encoder has with a hidden unit $s$ in the decoder.
Also, the improvement on the BLEU score migth indicate that that the dot score has limits while capturing the energy between these two hidden states.
Finally, the graph show us an increase of perfect BLEU scores that migth be due short and easy translations examples where the Bahdanau attention capture the conditional probability for more of this examples compared with the soft attention of the book.</p><pre><code class="python"># Bahdanau attention using Loung Decoder
evaluate_model_blue_4(bahdanau_attention_modelv2, dataset, args)
</code></pre><pre><code>0.4762533804201139 0.4784306445040927
</code></pre><p><img src="/img/javier_antonio_gonzalez_trejo_attention_34_1.png" alt="png" /></p><p>Here we see that using that the two models almost perfom the same. Thus, for a fair comparision, we will use the Bahdanau attention with Loung decoder for fair comparsion with the other attention models, since they use the Loung Decoder as well.</p><h2 id="2-visualize-bahdanau-soft-attention">2) Visualize Bahdanau Soft Attention.</h2><p>Next, we will visualize the attention between different hidden states. We will compare both Bahdanau soft attention using the Loung decoder and the soft attention from the book.</p><pre><code class="python"># Used to visualize one attention model at a time
def get_top_results_idx(model, dataset):
    dataset.set_split('val')
    vectorizer = dataset.get_vectorizer()
    batch_generator = generate_nmt_batches(dataset,
                                           batch_size=32,
                                           device=device,
                                           shuffle=False)
    batch_dict = next(batch_generator)

    model = soft_attention_model.eval().to(device)
    sampler = NMTSampler(vectorizer, model)
    sampler.apply_to_batch(batch_dict)

    all_results = []
    for i in range(32):
        all_results.append([i, sampler.get_ith_item(i, False)])

    index = [i for i, x in all_results if x['bleu-4']&gt;0.5]

    return index
</code></pre><pre><code class="python"># Refactored function used to visualize attention models
# Was required in order to obtain the same sample sequences
# across different models for better performance comparision
def visualize_attention(model, dataset, name, index):
    dataset.set_split('val')
    vectorizer = dataset.get_vectorizer()
    batch_generator = generate_nmt_batches(dataset,
                                           batch_size=32,
                                           device=device,
                                           shuffle=False)
    batch_dict = next(batch_generator)

    model = model.eval().to(device)
    sampler = NMTSampler(vectorizer, model)
    sampler.apply_to_batch(batch_dict)

    all_results = []
    for i in range(32):
        all_results.append([i, sampler.get_ith_item(i, False)])

    top_results = [x for i, x in all_results if i in index]

    for i, sample in enumerate(top_results):
        plt.figure()
        target_len = len(sample['sampled'])
        source_len = len(sample['source'])

        attention_matrix = sample['attention'][:target_len, :source_len+2].transpose()#[::-1]
        ax = sns.heatmap(attention_matrix, center=0.0)
        ylabs = ["&lt;BOS&gt;"]+sample['source']+["&lt;EOS&gt;"]
        #ylabs = sample['source']
        #ylabs = ylabs[::-1]
        ax.set_yticklabels(ylabs, rotation=0)
        ax.set_xticklabels(sample['sampled'], rotation=90)
        ax.set_xlabel("Target Sentence")
        ax.set_ylabel("Source Sentence\n\n")
        ax.title.set_text("Figure {}-{} {:.2f}".format(name, i, sample['bleu-4']))

</code></pre><pre><code class="python">index = get_top_results_idx(bahdanau_attention_modelv2, dataset)
visualize_attention(bahdanau_attention_modelv2, dataset, "Bahdanau", index)
</code></pre><p><img src="/img/javier_antonio_gonzalez_trejo_attention_39_0.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_39_1.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_39_2.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_39_3.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_39_4.png" alt="png" /></p><p>Here we see a monotonic alignment between the different tokens. An interesting behavior shown in the Figure Bahdanau is that while the increse of the spreding in the attention compared with the other figures, which migth be due the increase of the sequecne to translate.</p><h2 id="3-visual-comparision-between-the-attentions">3) Visual comparision between the attentions</h2><p>Next, we will compare the attention on the same sequences. For a fair comparision, we will evaluate 5 sequences of the validation set at random.</p><pre><code class="python"># Prefered for a fair comparision between two or more attention models
def get_random_sample_idx(samples):
    return rnd.sample(range(32), samples % 32)
</code></pre><pre><code class="python">index = get_random_sample_idx(5)
visualize_attention(bahdanau_attention_modelv2, dataset, "Bahdanau", index)
visualize_attention(soft_attention_model, dataset, "Soft", index)
</code></pre><p><img src="/img/javier_antonio_gonzalez_trejo_attention_43_0.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_43_1.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_43_2.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_43_3.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_43_4.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_43_5.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_43_6.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_43_7.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_43_8.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_43_9.png" alt="png" /></p><p>From randomly chosen sample sequences, the Bahdanau performs the best with highlights in the the Figures Bahdanau-3 and Bahdanau-2, which outperformed by a huge amount the Soft Attention of the book.
The most interesting aspect is that the attention is monotonic for the Bahdanau attention, while the Soft attention of the book has the attention scores all over the matrix. This might be an indication that a monotonic alignment is a better representation of the relationship between the hidden states of the encoder and a hidden state of the decoder at time $t$. Also, the lack of monotonic alignment using dot scores might be because the dot product is not cable of representing this alignments, and thus, will tend to fail more compared with the Bahdanau implementation. Finally, Bahdanau soft attention implementation seems to perform better overall.</p><h2 id="4-local-attention">4) Local attention</h2><p>Local attention is a combination between soft attention and hard attention, instead of obtaining the attention over all the hidden states or only one, the attention is spread over a fixed window. Loung et. al in their article, argue the combination has 2 benefits: The first one is that the computational cost when translating words of considerable length, where the global attention for many hidden states may be impractical. And to have a differentiable/easier to train function compared with plain hard attention, which is his naive form is non differentiable or requires complex training methods like Monte Carlo gradient approximations and reinforcement learning, which is an open problem by itself.</p><p>For this, first we need to find the predict where the alignment will be centered (local-p predective alignment), for this we have the following:
$$ p_t = S \cdot sigmoid(v_{p}^{T} tanh(W_{p}s_t)) $$</p><p>Next, we need to select the hidden states around this point using a window of size $D$, to make this "selection" differentiable, we multiply the normalized score of the hidden state $s_t$ and the hidden state $h_s$ where "s" is the s hidden state of the encoder, and multiply them by a 1D Gaussian at point $s$ with center at "p_t" and standar deviation of "\frac{D}{2}", the full equition is as follows:
$$ a_t(s) = align(s_{t}, h{s})exp(- \frac{(s-p_t)<sup>2}{2\sigma</sup>2}) $$
$$ align(s_{t}, h_{s}) = softmax(s_{t}W_{a}h_{s}) $$</p><p>We implement the local attention using the "general" score, since is reported to work the best in the article by Loung (Effective Approaches to Attention-based Neural Machine Translation)</p><pre><code class="python">class LocalAttention(AttentionInterface):
    """Implementation of Loung Local attention. The main objective here, is
    to align the hidden state $s_t$ of the encoder with the encoder hidden states
    around a window D with center in $p_t$ which value is lerned

      Args:
       num_features: Dimentionality of the features of both hidden states $h$ and $s$ to calculate the alignment.
       size_encoder_vector: Size of the hidden state $h$
       size_query_vector: Size of the hidden state $s$
       d: size of the window
    """

    def __init__(self, num_features, size_encoder_vector, size_query_vector, d):
        super(LocalAttention, self).__init__()

        self.p_weigths         = nn.Linear(size_query_vector, num_features)
        self.v_weigths         = nn.Linear(num_features, 1)

        self.attention_weigths = nn.Linear(size_encoder_vector,
                                           size_query_vector)
        self.d                 = d

    def get_center_point(self, query_vector, num_encoder_hidden):
        """Obtains the point $p_t$"""

        p_features = self.p_weigths(query_vector)
        v_features = self.v_weigths(F.tanh(p_features))

        return num_encoder_hidden * torch.sigmoid(v_features)

    def get_gaussian_vector(self, p_t, num_encoder_hidden, batch_size):
        variance = torch.tensor([(self.d / 2.) ** 2.]).to(device)

        index = range(num_encoder_hidden)

        position_vector = torch.tensor(index).unsqueeze(0).repeat(batch_size, 1)\
            .to(device)

        position_vector = torch.exp(- ((position_vector - p_t) ** 2.) /\
                                    (2. * variance))
        return position_vector

    def calculate_score(self, query_vector, encoder_hidden_vectors):
        tmp_encoder_vectors = self.attention_weigths(encoder_hidden_vectors)
        # Tranpose to [batch, size_query_vector, num_encoder_hidden]
        tmp_encoder_vectors = tmp_encoder_vectors.transpose(1, 2)

        tmp_query_vectors = query_vector.unsqueeze(dim=1)

        energy = torch.matmul(tmp_query_vectors, tmp_encoder_vectors)

        # Transpose to [batch, num_encoder_hidden, 1]
        return energy.transpose(1, 2).squeeze()

    def forward(self, query_vector, encoder_state_vectors):
        batch_size, num_encoder_hidden, _ = encoder_state_vectors.shape

        p_t = self.get_center_point(query_vector, num_encoder_hidden)
        position_vector = self.get_gaussian_vector(p_t, num_encoder_hidden,
                                                   batch_size)

        vector_scores = self.calculate_score(query_vector,
                                             encoder_state_vectors)

        vector_probabilities = F.softmax(vector_scores, dim = -1) *\
            position_vector

        context_vectors = torch.matmul(encoder_state_vectors.transpose(-2, -1),
                                       vector_probabilities.unsqueeze(dim=2))\
                               .squeeze()

        return context_vectors, vector_probabilities
</code></pre><h3 id="training-local-attention">Training Local Attention</h3><p>First, we will train the model stting $D = 5$ since our sequences are short.</p><pre><code class="python">args = Namespace(dataset_csv="data/nmt/simplest_eng_fra.csv",
                 vectorizer_file="vectorizer.json",
                 model_state_file="local_attention.pth",
                 save_dir="model_storage/attention",
                 reload_from_files=False,
                 expand_filepaths_to_save_dir=True,
                 cuda=True,
                 seed=1337,
                 learning_rate=5e-4,
                 batch_size=32,
                 num_epochs=100,
                 early_stopping_criteria=5,
                 source_embedding_size=24,
                 target_embedding_size=24,
                 encoding_size=32,
                 catch_keyboard_interrupt=True,
                 attention=LocalAttention,
                 attention_init_args=[64, 64, 64, 5],
                 decoder=NMTDecoder)

# handle dirs
handle_dirs(args.save_dir)

args = expand_filepaths_to_save_dir(args)

dataset = create_dataset(args)
vectorizer = dataset.get_vectorizer()

local_attention_model =\
    NMTModel(source_vocab_size=len(vectorizer.source_vocab),
             source_embedding_size=args.source_embedding_size,
             target_vocab_size=len(vectorizer.target_vocab),
             target_embedding_size=args.target_embedding_size, 
             encoding_size=args.encoding_size,
             target_bos_index=vectorizer.target_vocab.begin_seq_index,
             attention=args.attention,
             attention_init_args=args.attention_init_args,
             decoder=args.decoder)
</code></pre><pre><code>Expanded filepaths: 
	model_storage/attention/vectorizer.json
	model_storage/attention/local_attention.pth
</code></pre><pre><code class="python">#local_attention_model,\
#    local_attention_state = train(args, dataset, local_attention_model)
# Save the model
#torch.save(local_attention_model.state_dict(), args.model_state_file)
</code></pre><pre><code>Using CUDA: True
New model



HBox(children=(FloatProgress(value=0.0, description='training routine', style=ProgressStyle(description_width=…



HBox(children=(FloatProgress(value=0.0, description='split=train', max=285.0, style=ProgressStyle(description_…



HBox(children=(FloatProgress(value=0.0, description='split=val', max=61.0, style=ProgressStyle(description_wid…


 55%|█████▌    | 55/100 [15:49&lt;12:57, 17.27s/it]
</code></pre><pre><code class="python">local_attention_model.load_state_dict(torch.load(args.model_state_file))
</code></pre><pre><code>&lt;All keys matched successfully&gt;
</code></pre><p>For the sake of comparision, we will use the same $D$ as in the paper of Luong.</p><pre><code class="python">args = Namespace(dataset_csv="data/nmt/simplest_eng_fra.csv",
                 vectorizer_file="vectorizer.json",
                 model_state_file="local_attentionv2.pth",
                 save_dir="model_storage/attention",
                 reload_from_files=False,
                 expand_filepaths_to_save_dir=True,
                 cuda=True,
                 seed=1337,
                 learning_rate=5e-4,
                 batch_size=32,
                 num_epochs=100,
                 early_stopping_criteria=5,
                 source_embedding_size=24,
                 target_embedding_size=24,
                 encoding_size=32,
                 catch_keyboard_interrupt=True,
                 attention=LocalAttention,
                 attention_init_args=[64, 64, 64, 10],
                 decoder=NMTDecoder)

# handle dirs
handle_dirs(args.save_dir)

args = expand_filepaths_to_save_dir(args)

dataset = create_dataset(args)
vectorizer = dataset.get_vectorizer()

local_attention_modelv2 =\
    NMTModel(source_vocab_size=len(vectorizer.source_vocab),
             source_embedding_size=args.source_embedding_size,
             target_vocab_size=len(vectorizer.target_vocab),
             target_embedding_size=args.target_embedding_size, 
             encoding_size=args.encoding_size,
             target_bos_index=vectorizer.target_vocab.begin_seq_index,
             attention=args.attention,
             attention_init_args=args.attention_init_args,
             decoder=args.decoder)
</code></pre><pre><code>Expanded filepaths: 
	model_storage/attention/vectorizer.json
	model_storage/attention/local_attentionv2.pth
</code></pre><pre><code class="python">#local_attention_modelv2,\
#    local_attention_statev2 = train(args, dataset, local_attention_modelv2)
# Save the model
#torch.save(local_attention_modelv2.state_dict(), args.model_state_file)
</code></pre><pre><code>Using CUDA: True
New model



HBox(children=(FloatProgress(value=0.0, description='training routine', style=ProgressStyle(description_width=…



HBox(children=(FloatProgress(value=0.0, description='split=train', max=285.0, style=ProgressStyle(description_…



HBox(children=(FloatProgress(value=0.0, description='split=val', max=61.0, style=ProgressStyle(description_wid…


100%|██████████| 100/100 [29:11&lt;00:00, 17.51s/it]
</code></pre><pre><code class="python">local_attention_modelv2.load_state_dict(torch.load(args.model_state_file))
</code></pre><pre><code>&lt;All keys matched successfully&gt;
</code></pre><h2 id="6-performance-comparision-betten-local-attention-vs-soft-attention-of-the-book-and-bahdanau-soft-attention">6) Performance comparision betten Local attention vs soft attention of the book and Bahdanau soft attention</h2><p>Next, we will proceed to evaluate the performance of the two trained models.</p><pre><code class="python">evaluate_model_blue_4(local_attention_model, dataset, args)
</code></pre><pre><code>0.44007704190743885 0.4371876689838262
</code></pre><p><img src="/img/javier_antonio_gonzalez_trejo_attention_56_1.png" alt="png" /></p><pre><code class="python">evaluate_model_blue_4(local_attention_modelv2, dataset, args)
</code></pre><pre><code>0.3478126923950574 0.33662225205617236
</code></pre><p><img src="/img/javier_antonio_gonzalez_trejo_attention_57_1.png" alt="png" /></p><pre><code class="python">evaluate_model_blue_4(bahdanau_attention_modelv2, dataset, args)
</code></pre><pre><code>0.4776666979044746 0.47347182949514793
</code></pre><p><img src="/img/javier_antonio_gonzalez_trejo_attention_58_1.png" alt="png" /></p><pre><code class="python">evaluate_model_blue_4(soft_attention_model, dataset, args)
</code></pre><pre><code>0.40910155454280084 0.41534829911631765
</code></pre><p><img src="/img/javier_antonio_gonzalez_trejo_attention_59_1.png" alt="png" /></p><p>First, it seems that the model decreeses it's performance when a large $D$ is used. This migth be due since at a larger window, the attention model start to act as the global attention proposed by Loung, which he argues has wost performance than local attention.
Overall, the attention model performs only better than the global attention proposed in the book, and worst than the soft attention of Bahdanau.</p><h2 id="5-visualize-local-attention">5) Visualize local attention</h2><pre><code class="python">index = get_top_results_idx(local_attention_model, dataset)
visualize_attention(local_attention_model, dataset, "Local", index)
</code></pre><p><img src="/img/javier_antonio_gonzalez_trejo_attention_62_0.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_62_1.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_62_2.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_62_3.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_62_4.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_62_5.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_62_6.png" alt="png" /></p><p>In this images, we see monotonic and no monotonic relationships as expected, since we local attention allows to not necessarily use that kind of relationships. An example is figure LOCAL-2, which displays a BLEU score of 0.75 and its attention is no monotonic, which might indicate that for some sequences is not the ideal type of relationship.
We also see good performance for sequences of 5 or more word (mostly over 0.4 BLUE score).</p><h2 id="6-visual-comparision-betten-local-attention-vs-soft-attention-of-the-book-and-bahdanau-soft-attention">6) Visual comparision betten Local attention vs soft attention of the book and Bahdanau soft attention</h2><pre><code class="python">index = get_random_sample_idx(5)
visualize_attention(bahdanau_attention_modelv2, dataset, "Bahdanau", index)
visualize_attention(soft_attention_model, dataset, "Soft", index)
visualize_attention(local_attention_model, dataset, "Local", index)
</code></pre><p><img src="/img/javier_antonio_gonzalez_trejo_attention_65_0.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_65_1.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_65_2.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_65_3.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_65_4.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_65_5.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_65_6.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_65_7.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_65_8.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_65_9.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_65_10.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_65_11.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_65_12.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_65_13.png" alt="png" /></p><p><img src="/img/javier_antonio_gonzalez_trejo_attention_65_14.png" alt="png" /></p><p>As we seen before, soft attention of the book does not perform better in any of the sequences out of the three.
Due this, it will be more interesting the comparison between Bahdanau and Local attention models.
The performance between them are very similar. Bahdanau attention has the best translations, while Local attention maintains a good performance over all. Looking at the Figures 0, which is the largest of the set, Bahdanau performs reasonable while Local attention fails considerable. In the other hand, we see Figure Bahdanau-2 fails completely while local attention has a reasonable score. Finally, in Figure Bahdanau-3 has the best translation while Local attention does not provide a reasonable one.</p><h2 id="attention-model-conclusions">Attention model conclusions</h2><p>Having compared the attention models ant their performance, it is difficult to assess the argument of the author on behalf of the better performance for translation in big sequences, that being paragraphs to documents. Observing that Bahdanau attention with the Loung decoder (using the current decoder hidden state), and since the next task is limited to sequences of length 20-40, which are larger than the sequences used in this work but still not as big as a paragraph or document. It appears more reasonable to use Bahdanau attention model instead of the Local model in the next task.</p>
</div>

<div id="post-tags">
    <b>Tags: </b>
    
    <a href="/tags-output/nlp/">nlp</a>
    
    <a href="/tags-output/deep learning/">deep learning</a>
    
    <a href="/tags-output/gru/">gru</a>
    
    <a href="/tags-output/attention/">attention</a>
    
</div>


    

    <div id="prev-next">
        
        
        <a class="right" href="/posts-output/2020-10-22-grad_cam/">GRAD-CAM &raquo;</a>
        
    </div>
</div>

            </div>
        </div>
    </div>
    <footer>Copyright &copy; 2020 Javier Antonio Gonzalez-Trejo
        <p style="text-align: center;">Powered by <a href="http://cryogenweb.org">Cryogen</a></p></footer>
</div>
<script src="//code.jquery.com/jquery-1.11.0.min.js"></script>
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.0/js/bootstrap.min.js"></script>
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script src="/js/highlight.pack.js" type="application/javascript"></script>
<script>hljs.initHighlightingOnLoad();</script>


</body>
</html>
